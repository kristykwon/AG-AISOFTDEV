{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 11:11:09,172 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "\"\"\"\n",
      "This module contains \"happy path\" tests for the Onboarding Task Management API.\n",
      "\n",
      "It uses the FastAPI TestClient to send requests to the application and verify\n",
      "the successful creation and retrieval of users. Each test is designed to be\n",
      "independent and follows the Arrange-Act-Assert pattern.\n",
      "\"\"\"\n",
      "\n",
      "import datetime\n",
      "import uuid\n",
      "\n",
      "from fastapi.testclient import TestClient\n",
      "\n",
      "# Assuming the application code is in a file named `main.py`.\n",
      "# If your file is named differently, please adjust the import accordingly.\n",
      "from main import app\n",
      "\n",
      "# Create a TestClient instance to interact with the FastAPI application\n",
      "client = TestClient(app)\n",
      "\n",
      "\n",
      "def test_create_user_success():\n",
      "    \"\"\"\n",
      "    Tests the successful creation of a new user via the POST /users endpoint.\n",
      "\n",
      "    It first creates a prerequisite role, then sends a valid user payload.\n",
      "    It asserts that the response status is 201 (Created) and that the\n",
      "    returned user data matches the input data.\n",
      "    \"\"\"\n",
      "    # Arrange\n",
      "    # 1. Create a prerequisite Role to satisfy the foreign key constraint.\n",
      "    # A unique role name is used to ensure test independence.\n",
      "    role_payload = {\"role_name\": f\"Test Role {uuid.uuid4()}\"}\n",
      "    role_response = client.post(\"/roles\", json=role_payload)\n",
      "    assert role_response.status_code == 201, \"Failed to create prerequisite role\"\n",
      "    role_id = role_response.json()[\"role_id\"]\n",
      "\n",
      "    # 2. Define the new user payload with a unique email to avoid conflicts.\n",
      "    unique_email = f\"testuser_{uuid.uuid4()}@example.com\"\n",
      "    start_date = datetime.date.today().isoformat()\n",
      "    new_user_payload = {\n",
      "        \"first_name\": \"Jane\",\n",
      "        \"last_name\": \"Doe\",\n",
      "        \"email\": unique_email,\n",
      "        \"role_id\": role_id,\n",
      "        \"start_date\": start_date,\n",
      "    }\n",
      "\n",
      "    # Act\n",
      "    response = client.post(\"/users\", json=new_user_payload)\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 201, \"Expected status code 201 (Created)\"\n",
      "    created_user = response.json()\n",
      "\n",
      "    assert \"user_id\" in created_user, \"Response should contain a user_id\"\n",
      "    assert isinstance(created_user[\"user_id\"], int)\n",
      "    assert created_user[\"first_name\"] == new_user_payload[\"first_name\"]\n",
      "    assert created_user[\"last_name\"] == new_user_payload[\"last_name\"]\n",
      "    assert created_user[\"email\"] == new_user_payload[\"email\"]\n",
      "    assert created_user[\"role_id\"] == new_user_payload[\"role_id\"]\n",
      "    assert created_user[\"start_date\"] == new_user_payload[\"start_date\"]\n",
      "\n",
      "\n",
      "def test_get_all_users_success():\n",
      "    \"\"\"\n",
      "    Tests retrieving a list of all users via the GET /users endpoint.\n",
      "\n",
      "    It sends a GET request and asserts that the response status is 200 (OK)\n",
      "    and the response body is a list. It also verifies the structure of user\n",
      "    objects within the list if it's not empty.\n",
      "    \"\"\"\n",
      "    # Arrange\n",
      "    # No specific arrangement is needed as the endpoint should work correctly\n",
      "    # even with an empty database, returning an empty list.\n",
      "\n",
      "    # Act\n",
      "    response = client.get(\"/users\")\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 200, \"Expected status code 200 (OK)\"\n",
      "    users_list = response.json()\n",
      "\n",
      "    assert isinstance(users_list, list), \"Response should be a list\"\n",
      "    if users_list:\n",
      "        # If the list is not empty, check the structure of the first user object\n",
      "        first_user = users_list[0]\n",
      "        assert \"user_id\" in first_user\n",
      "        assert \"first_name\" in first_user\n",
      "        assert \"last_name\" in first_user\n",
      "        assert \"email\" in first_user\n",
      "        assert \"role_id\" in first_user\n",
      "        assert \"start_date\" in first_user\n",
      "\n",
      "\n",
      "def test_get_specific_user_success():\n",
      "    \"\"\"\n",
      "    Tests retrieving a single, specific user by ID via GET /users/{user_id}.\n",
      "\n",
      "    It first creates a new user to ensure a known user exists. Then, it uses\n",
      "    the ID from the creation response to fetch that specific user.\n",
      "    It asserts that the response status is 200 (OK) and the retrieved data\n",
      "    matches the data of the user that was created.\n",
      "    \"\"\"\n",
      "    # Arrange\n",
      "    # 1. Create a prerequisite Role.\n",
      "    role_payload = {\"role_name\": f\"Test Role {uuid.uuid4()}\"}\n",
      "    role_response = client.post(\"/roles\", json=role_payload)\n",
      "    assert role_response.status_code == 201, \"Failed to create prerequisite role\"\n",
      "    role_id = role_response.json()[\"role_id\"]\n",
      "\n",
      "    # 2. Create a new user to be fetched later.\n",
      "    unique_email = f\"fetch_user_{uuid.uuid4()}@example.com\"\n",
      "    start_date = (datetime.date.today() - datetime.timedelta(days=10)).isoformat()\n",
      "    user_to_create_payload = {\n",
      "        \"first_name\": \"Fetch\",\n",
      "        \"last_name\": \"Me\",\n",
      "        \"email\": unique_email,\n",
      "        \"role_id\": role_id,\n",
      "        \"start_date\": start_date,\n",
      "    }\n",
      "    create_response = client.post(\"/users\", json=user_to_create_payload)\n",
      "    assert create_response.status_code == 201, \"Failed to create user for fetching\"\n",
      "    created_user_data = create_response.json()\n",
      "    user_id_to_fetch = created_user_data[\"user_id\"]\n",
      "\n",
      "    # Act\n",
      "    response = client.get(f\"/users/{user_id_to_fetch}\")\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 200, \"Expected status code 200 (OK)\"\n",
      "    fetched_user_data = response.json()\n",
      "\n",
      "    # Verify that the fetched user data is identical to the created user data\n",
      "    assert fetched_user_data == created_user_data, \"Fetched data does not match created data\"\n"
     ]
    }
   ],
   "source": [
    "# TWrite a prompt to generate happy path tests for your API.\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are an expert QA Engineer specializing in pytest and FastAPI testing. Your task is to generate comprehensive \"happy path\" test functions for a FastAPI application.\n",
    "\n",
    "**Context - Application Code:**\n",
    "{app_code}\n",
    "\n",
    "**Instructions:**\n",
    "Generate a pytest test file with the following requirements:\n",
    "\n",
    "1. **Test Structure:**\n",
    "   - Use pytest best practices with clear, descriptive test function names following the pattern `test_<operation>_<expected_outcome>`\n",
    "   - Include proper imports: `pytest`, `fastapi.testclient.TestClient`, and import the `app` from `app.main`\n",
    "   - Create a module-level TestClient instance: `client = TestClient(app)`\n",
    "\n",
    "2. **Required Happy Path Tests:**\n",
    "   a) **Test POST /users/** - Create a new user successfully:\n",
    "      - Send a valid user payload with all required fields (first_name, last_name, email, role_id, start_date)\n",
    "      - Assert the response status code is 201 (Created)\n",
    "      - Assert the response contains the created user data with a user_id\n",
    "      - Assert all input fields match the response fields\n",
    "      - Use a unique email address (e.g., with timestamp or UUID) to avoid conflicts\n",
    "   \n",
    "   b) **Test GET /users/** - List all users:\n",
    "      - Send a GET request to retrieve all users\n",
    "      - Assert the response status code is 200 (OK)\n",
    "      - Assert the response is a list (can be empty or contain users)\n",
    "      - If users exist, verify each user has the required fields (user_id, first_name, last_name, email, role_id, start_date)\n",
    "   \n",
    "   c) **Test GET /users/{{user_id}}** - Get a specific user:\n",
    "      - First create a user via POST /users/\n",
    "      - Then retrieve that user by their user_id\n",
    "      - Assert the response status code is 200 (OK)\n",
    "      - Assert the retrieved user data matches the created user data\n",
    "\n",
    "3. **Best Practices to Follow:**\n",
    "   - Each test should be independent and not rely on specific database state\n",
    "   - Use descriptive variable names\n",
    "   - Add docstrings to each test function explaining what it tests\n",
    "   - Use proper assertions with clear failure messages\n",
    "   - Clean up any test data if necessary (though fixtures will handle this later)\n",
    "   - Follow the Arrange-Act-Assert (AAA) pattern in each test\n",
    "\n",
    "4. **Output Format:**\n",
    "   - Provide only valid Python code for the test file\n",
    "   - Do not include markdown code fences or explanatory text outside the code\n",
    "   - Include a module-level docstring explaining the purpose of the test file\n",
    "   - Ensure the code is properly formatted and follows PEP 8 style guidelines\n",
    "\n",
    "Generate the complete, production-ready pytest test file now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\", overwrite = True)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "\"\"\"\n",
      "Module for edge case and error handling tests for the Onboarding Task Management API.\n",
      "\n",
      "This file contains pytest tests that specifically target scenarios where the API\n",
      "is expected to return an error response, such as creating duplicate resources,\n",
      "requesting non-existent items, or providing invalid foreign keys.\n",
      "\"\"\"\n",
      "import datetime\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from sqlalchemy.pool import StaticPool\n",
      "\n",
      "# Assuming the application code is in a file named `main.py` in the parent directory\n",
      "# or a location accessible via PYTHONPATH.\n",
      "from main import app, get_db, Base\n",
      "\n",
      "# --- Test Database Setup ---\n",
      "# Use an in-memory SQLite database for isolated testing\n",
      "SQLALCHEMY_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "\n",
      "engine = create_engine(\n",
      "    SQLALCHEMY_DATABASE_URL,\n",
      "    connect_args={\"check_same_thread\": False},\n",
      "    poolclass=StaticPool,\n",
      ")\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "\n",
      "# --- Fixtures ---\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def db_session():\n",
      "    \"\"\"\n",
      "    Pytest fixture to create a new database session for each test function.\n",
      "    It creates all tables before the test and drops them afterwards, ensuring isolation.\n",
      "    \"\"\"\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    db = TestingSessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "        Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client(db_session):\n",
      "    \"\"\"\n",
      "    Pytest fixture to override the `get_db` dependency with a test database\n",
      "    session and provide a TestClient instance for making API requests.\n",
      "    \"\"\"\n",
      "    def override_get_db():\n",
      "        try:\n",
      "            yield db_session\n",
      "        finally:\n",
      "            db_session.close()\n",
      "\n",
      "    app.dependency_overrides[get_db] = override_get_db\n",
      "    yield TestClient(app)\n",
      "    # Clean up the override after the test function completes\n",
      "    app.dependency_overrides.clear()\n",
      "\n",
      "\n",
      "# --- Helper Fixtures to Create Prerequisite Data ---\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def test_role(client: TestClient) -> dict:\n",
      "    \"\"\"Fixture to create a standard role for use in other tests.\"\"\"\n",
      "    response = client.post(\"/roles\", json={\"role_name\": \"Test Engineer\"})\n",
      "    assert response.status_code == 201, \"Failed to create prerequisite role\"\n",
      "    return response.json()\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def test_user(client: TestClient, test_role: dict) -> dict:\n",
      "    \"\"\"Fixture to create a standard user for use in other tests.\"\"\"\n",
      "    user_payload = {\n",
      "        \"first_name\": \"Test\",\n",
      "        \"last_name\": \"User\",\n",
      "        \"email\": \"test.user@example.com\",\n",
      "        \"role_id\": test_role[\"role_id\"],\n",
      "        \"start_date\": \"2024-01-01\",\n",
      "    }\n",
      "    response = client.post(\"/users\", json=user_payload)\n",
      "    assert response.status_code == 201, \"Failed to create prerequisite user\"\n",
      "    return response.json()\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def test_task(client: TestClient) -> dict:\n",
      "    \"\"\"Fixture to create a standard task for use in other tests.\"\"\"\n",
      "    task_payload = {\n",
      "        \"task_name\": \"Setup Dev Environment\",\n",
      "        \"description\": \"Install all necessary software.\",\n",
      "        \"is_mandatory\": True,\n",
      "        \"status\": \"pending\"\n",
      "    }\n",
      "    response = client.post(\"/tasks\", json=task_payload)\n",
      "    assert response.status_code == 201, \"Failed to create prerequisite task\"\n",
      "    return response.json()\n",
      "\n",
      "\n",
      "# --- User Endpoint Error Tests ---\n",
      "\n",
      "def test_create_user_with_duplicate_email(client: TestClient, test_role: dict):\n",
      "    \"\"\"\n",
      "    Tests that creating a user with an email that already exists\n",
      "    results in a 400 Bad Request error.\n",
      "    \"\"\"\n",
      "    # Arrange: Create an initial user\n",
      "    user_payload = {\n",
      "        \"first_name\": \"Jane\",\n",
      "        \"last_name\": \"Doe\",\n",
      "        \"email\": \"jane.doe.duplicate@example.com\",\n",
      "        \"role_id\": test_role[\"role_id\"],\n",
      "        \"start_date\": datetime.date.today().isoformat(),\n",
      "    }\n",
      "    first_response = client.post(\"/users\", json=user_payload)\n",
      "    assert first_response.status_code == 201, \"Initial user creation failed\"\n",
      "\n",
      "    # Act: Attempt to create another user with the same email\n",
      "    second_response = client.post(\"/users\", json=user_payload)\n",
      "\n",
      "    # Assert: Verify the error response\n",
      "    assert second_response.status_code == 400\n",
      "    assert second_response.json() == {\"detail\": \"Email already registered\"}\n",
      "\n",
      "\n",
      "def test_get_user_with_non_existent_id(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests that requesting a user with an ID that does not exist\n",
      "    results in a 404 Not Found error.\n",
      "    \"\"\"\n",
      "    # Arrange: A non-existent user ID\n",
      "    non_existent_user_id = 999999\n",
      "\n",
      "    # Act: Send a GET request for the non-existent user\n",
      "    response = client.get(f\"/users/{non_existent_user_id}\")\n",
      "\n",
      "    # Assert: Verify the 404 response\n",
      "    assert response.status_code == 404\n",
      "    assert response.json() == {\"detail\": \"User not found\"}\n",
      "\n",
      "\n",
      "def test_create_user_with_non_existent_role_id(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests that creating a user with a `role_id` that does not exist\n",
      "    results in a 400 Bad Request error.\n",
      "    \"\"\"\n",
      "    # Arrange: User data with a role_id that is not in the database\n",
      "    non_existent_role_id = 999\n",
      "    user_payload = {\n",
      "        \"first_name\": \"No\",\n",
      "        \"last_name\": \"Role\",\n",
      "        \"email\": \"no.role@example.com\",\n",
      "        \"role_id\": non_existent_role_id,\n",
      "        \"start_date\": \"2024-02-01\",\n",
      "    }\n",
      "\n",
      "    # Act: Attempt to create the user\n",
      "    response = client.post(\"/users\", json=user_payload)\n",
      "\n",
      "    # Assert: Verify the 400 error\n",
      "    assert response.status_code == 400\n",
      "    assert response.json() == {\n",
      "        \"detail\": f\"Role with role_id {non_existent_role_id} not found\"\n",
      "    }\n",
      "\n",
      "\n",
      "def test_update_user_to_existing_email(client: TestClient, test_role: dict):\n",
      "    \"\"\"\n",
      "    Tests that updating a user's email to an email that is already\n",
      "    in use by another user results in a 400 Bad Request error.\n",
      "    \"\"\"\n",
      "    # Arrange: Create two distinct users\n",
      "    user1_payload = {\n",
      "        \"first_name\": \"First\", \"last_name\": \"User\",\n",
      "        \"email\": \"first.user@example.com\", \"role_id\": test_role[\"role_id\"],\n",
      "        \"start_date\": \"2024-03-01\"\n",
      "    }\n",
      "    user1_response = client.post(\"/users\", json=user1_payload)\n",
      "    user1_id = user1_response.json()[\"user_id\"]\n",
      "\n",
      "    user2_payload = {\n",
      "        \"first_name\": \"Second\", \"last_name\": \"User\",\n",
      "        \"email\": \"second.user@example.com\", \"role_id\": test_role[\"role_id\"],\n",
      "        \"start_date\": \"2024-03-02\"\n",
      "    }\n",
      "    client.post(\"/users\", json=user2_payload)\n",
      "\n",
      "    # Act: Attempt to update user1's email to user2's email\n",
      "    update_payload = {\"email\": \"second.user@example.com\"}\n",
      "    response = client.put(f\"/users/{user1_id}\", json=update_payload)\n",
      "\n",
      "    # Assert: Verify the 400 error\n",
      "    assert response.status_code == 400\n",
      "    assert response.json() == {\"detail\": \"Email already registered\"}\n",
      "\n",
      "\n",
      "def test_delete_user_with_non_existent_id(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests that attempting to delete a user with an ID that does not exist\n",
      "    results in a 404 Not Found error.\n",
      "    \"\"\"\n",
      "    # Arrange\n",
      "    non_existent_user_id = 999999\n",
      "\n",
      "    # Act\n",
      "    response = client.delete(f\"/users/{non_existent_user_id}\")\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 404\n",
      "    assert response.json() == {\"detail\": \"User not found\"}\n",
      "\n",
      "\n",
      "# --- Role Endpoint Error Tests ---\n",
      "\n",
      "def test_create_role_with_duplicate_name(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests that creating a role with a name that already exists\n",
      "    results in a 400 Bad Request error.\n",
      "    \"\"\"\n",
      "    # Arrange: Create an initial role\n",
      "    role_payload = {\"role_name\": \"Duplicate Role Name\"}\n",
      "    client.post(\"/roles\", json=role_payload)\n",
      "\n",
      "    # Act: Attempt to create another role with the same name\n",
      "    response = client.post(\"/roles\", json=role_payload)\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 400\n",
      "    assert response.json() == {\"detail\": \"Role name already exists\"}\n",
      "\n",
      "\n",
      "def test_get_role_with_non_existent_id(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests that requesting a role with an ID that does not exist\n",
      "    results in a 404 Not Found error.\n",
      "    \"\"\"\n",
      "    # Arrange\n",
      "    non_existent_role_id = 999999\n",
      "\n",
      "    # Act\n",
      "    response = client.get(f\"/roles/{non_existent_role_id}\")\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 404\n",
      "    assert response.json() == {\"detail\": \"Role not found\"}\n",
      "\n",
      "\n",
      "# --- Task Assignment Endpoint Error Tests ---\n",
      "\n",
      "def test_assign_task_with_non_existent_user_id(client: TestClient, test_task: dict):\n",
      "    \"\"\"\n",
      "    Tests that creating a task assignment with a `user_id` that does not exist\n",
      "    results in a 400 Bad Request error.\n",
      "    \"\"\"\n",
      "    # Arrange\n",
      "    non_existent_user_id = 999\n",
      "    assignment_payload = {\n",
      "        \"user_id\": non_existent_user_id,\n",
      "        \"task_id\": test_task[\"task_id\"]\n",
      "    }\n",
      "\n",
      "    # Act\n",
      "    response = client.post(\"/task_assignments\", json=assignment_payload)\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 400\n",
      "    assert response.json() == {\n",
      "        \"detail\": f\"User with user_id {non_existent_user_id} not found\"\n",
      "    }\n",
      "\n",
      "\n",
      "def test_assign_duplicate_task_to_user(client: TestClient, test_user: dict, test_task: dict):\n",
      "    \"\"\"\n",
      "    Tests that assigning the same task to the same user twice\n",
      "    results in a 400 Bad Request error due to the unique constraint.\n",
      "    \"\"\"\n",
      "    # Arrange: Create the first assignment\n",
      "    assignment_payload = {\n",
      "        \"user_id\": test_user[\"user_id\"],\n",
      "        \"task_id\": test_task[\"task_id\"]\n",
      "    }\n",
      "    first_response = client.post(\"/task_assignments\", json=assignment_payload)\n",
      "    assert first_response.status_code == 201, \"Initial assignment failed\"\n",
      "\n",
      "    # Act: Attempt to create the exact same assignment again\n",
      "    second_response = client.post(\"/task_assignments\", json=assignment_payload)\n",
      "\n",
      "    # Assert\n",
      "    assert second_response.status_code == 400\n",
      "    assert second_response.json() == {\n",
      "        \"detail\": \"This task is already assigned to this user\"\n",
      "    }\n",
      "\n",
      "\n",
      "def test_get_tasks_for_non_existent_user(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests that requesting task assignments for a user ID that does not exist\n",
      "    results in a 404 Not Found error.\n",
      "    \"\"\"\n",
      "    # Arrange\n",
      "    non_existent_user_id = 999999\n",
      "\n",
      "    # Act\n",
      "    response = client.get(f\"/users/{non_existent_user_id}/tasks\")\n",
      "\n",
      "    # Assert\n",
      "    assert response.status_code == 404\n",
      "    assert response.json() == {\"detail\": \"User not found\"}\n",
      "\n",
      "\n",
      "# --- Pydantic Validation Error Test ---\n",
      "\n",
      "def test_create_task_with_invalid_status_value(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests that creating a task with a status value not in the allowed list\n",
      "    ('pending', 'in_progress', 'completed') results in a 422 Unprocessable Entity error.\n",
      "    \"\"\"\n",
      "    # Arrange: Payload with an invalid status\n",
      "    invalid_task_payload = {\n",
      "        \"task_name\": \"Invalid Task\",\n",
      "        \"is_mandatory\": True,\n",
      "        \"status\": \"on_hold\"  # This is not a valid status\n",
      "    }\n",
      "\n",
      "    # Act\n",
      "    response = client.post(\"/tasks\", json=invalid_task_payload)\n",
      "\n",
      "    # Assert: FastAPI should automatically return a 422 for validation errors\n",
      "    assert response.status_code == 422\n",
      "    error_detail = response.json()[\"detail\"][0]\n",
      "    assert error_detail[\"type\"] == \"string_pattern_mismatch\"\n",
      "    assert \"status\" in error_detail[\"loc\"]\n",
      "    assert \"Input should match\" in error_detail[\"msg\"]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate edge case tests.\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are an expert QA Engineer specializing in pytest and FastAPI testing. Your task is to generate comprehensive edge case tests for a FastAPI application that handles error scenarios gracefully.\n",
    "\n",
    "**Context - Application Code:**\n",
    "{app_code}\n",
    "\n",
    "**Instructions:**\n",
    "Generate a pytest test file with edge case tests that verify proper error handling:\n",
    "\n",
    "1. **Test Structure:**\n",
    "   - Use pytest best practices with clear, descriptive test function names following the pattern `test_<operation>_<error_scenario>`\n",
    "   - Include proper imports: `pytest`, `fastapi.testclient.TestClient`, and import the `app` from `app.main`\n",
    "   - Create a module-level TestClient instance: `client = TestClient(app)`\n",
    "   - Add a module-level docstring explaining that this file contains edge case and error handling tests\n",
    "\n",
    "2. **Required Edge Case Tests:**\n",
    "\n",
    "   a) **Test POST /users/ with duplicate email** - Verify duplicate email handling:\n",
    "      - First, create a user with a specific email address\n",
    "      - Then, attempt to create another user with the same email\n",
    "      - Assert the second request returns status code 400 (Bad Request)\n",
    "      - Assert the error response contains a meaningful error message (e.g., \"Email already exists\")\n",
    "      - Include a docstring explaining this tests duplicate email constraint\n",
    "\n",
    "   b) **Test GET /users/{{user_id}} with non-existent ID** - Verify 404 handling:\n",
    "      - Send a GET request for a user_id that doesn't exist (e.g., 999999)\n",
    "      - Assert the response status code is 404 (Not Found)\n",
    "      - Assert the error response contains a meaningful error message (e.g., \"User not found\")\n",
    "      - Include a docstring explaining this tests missing resource handling\n",
    "\n",
    "3. **Best Practices to Follow:**\n",
    "   - Each test should be independent and properly isolated\n",
    "   - Use descriptive variable names that clearly indicate the test scenario\n",
    "   - Add comprehensive docstrings to each test function explaining what error condition it tests\n",
    "   - Use pytest.raises() where appropriate for exception testing (though FastAPI returns HTTP errors)\n",
    "   - Include clear assertion messages that help diagnose failures\n",
    "   - Follow the Arrange-Act-Assert (AAA) pattern\n",
    "   - For tests that create data first, ensure they use unique identifiers to avoid conflicts\n",
    "\n",
    "4. **Output Format:**\n",
    "   - Provide only valid Python code for the test file\n",
    "   - Do not include markdown code fences or explanatory text outside the code\n",
    "   - Include a comprehensive module-level docstring\n",
    "   - Ensure the code is properly formatted and follows PEP 8 style guidelines\n",
    "   - Group related tests logically with comments if helpful\n",
    "\n",
    "Generate the complete, production-ready pytest edge case test file now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Pytest DB Fixture ---\n",
      "\"\"\"\n",
      "conftest.py: Shared pytest fixtures for database-isolated testing.\n",
      "\n",
      "This file provides fixtures for setting up a clean, isolated, in-memory SQLite\n",
      "database for each test function. This ensures that tests are independent and\n",
      "do not interfere with each other or the production database.\n",
      "\n",
      "Fixtures provided:\n",
      "- `test_db`: Creates and tears down an in-memory SQLite database for each test.\n",
      "             It yields a SQLAlchemy session object for direct database interaction.\n",
      "- `client`: Depends on `test_db` to create a FastAPI `TestClient`. It overrides\n",
      "            the application's `get_db` dependency to use the isolated test\n",
      "            database session, allowing for full integration testing of API\n",
      "            endpoints with a clean database state.\n",
      "\"\"\"\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import Session, sessionmaker\n",
      "\n",
      "# Import the main application, the declarative base, and the dependency\n",
      "# Assuming the application code is in a file named `main.py` in the same directory\n",
      "# Adjust the import path if your file structure is different (e.g., `from app.main import ...`)\n",
      "from main import Base, app, get_db\n",
      "\n",
      "# Define the connection string for the in-memory SQLite database\n",
      "SQLALCHEMY_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def test_db() -> Session:\n",
      "    \"\"\"\n",
      "    Pytest fixture to create a fresh, isolated in-memory SQLite database for each test function.\n",
      "\n",
      "    This fixture handles the complete lifecycle of the test database:\n",
      "    1.  Creates a new SQLAlchemy engine configured for an in-memory SQLite DB.\n",
      "    2.  Creates all tables defined in the application's models (`Base.metadata`).\n",
      "    3.  Creates a new session (`db`) from a sessionmaker.\n",
      "    4.  Yields the session to the test function for use.\n",
      "    5.  After the test function completes (or fails), it tears down the database\n",
      "        by dropping all tables and closing the session.\n",
      "\n",
      "    Yields:\n",
      "        Session: A SQLAlchemy session object connected to the test database.\n",
      "    \"\"\"\n",
      "    # Create a new engine for the in-memory database\n",
      "    engine = create_engine(\n",
      "        SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n",
      "    )\n",
      "\n",
      "    # Create a sessionmaker configured for the test engine\n",
      "    TestingSessionLocal = sessionmaker(\n",
      "        autocommit=False, autoflush=False, bind=engine\n",
      "    )\n",
      "\n",
      "    # Create all tables in the database. This is the setup phase.\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "\n",
      "    db = TestingSessionLocal()\n",
      "\n",
      "    try:\n",
      "        # Yield the session to the test function\n",
      "        yield db\n",
      "    finally:\n",
      "        # Teardown phase: close the session and drop all tables\n",
      "        db.close()\n",
      "        Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client(test_db: Session) -> TestClient:\n",
      "    \"\"\"\n",
      "    Pytest fixture to create a FastAPI TestClient with an overridden database dependency.\n",
      "\n",
      "    This fixture depends on the `test_db` fixture to get an isolated database\n",
      "    session. It then overrides the main application's `get_db` dependency to\n",
      "    ensure that any API calls made through the TestClient use the isolated\n",
      "    test database instead of the production one.\n",
      "\n",
      "    This is the primary fixture to use for integration testing API endpoints.\n",
      "\n",
      "    Args:\n",
      "        test_db (Session): The SQLAlchemy session yielded by the `test_db` fixture.\n",
      "\n",
      "    Yields:\n",
      "        TestClient: An instance of `fastapi.testclient.TestClient` ready for making requests.\n",
      "    \"\"\"\n",
      "    # Define the dependency override function.\n",
      "    # This function will be called by FastAPI instead of the original `get_db`.\n",
      "    def override_get_db():\n",
      "        \"\"\"A simple generator to yield the test_db session.\"\"\"\n",
      "        try:\n",
      "            yield test_db\n",
      "        finally:\n",
      "            # The session is closed by the `test_db` fixture's teardown.\n",
      "            # No need to close it here.\n",
      "            pass\n",
      "\n",
      "    # Apply the dependency override to the FastAPI app.\n",
      "    app.dependency_overrides[get_db] = override_get_db\n",
      "\n",
      "    # Yield the TestClient instance for the test to use.\n",
      "    # The 'with' statement is not strictly necessary for TestClient but is good practice.\n",
      "    with TestClient(app) as test_client:\n",
      "        yield test_client\n",
      "\n",
      "    # Teardown: Clear the dependency overrides after the test is complete\n",
      "    # to ensure a clean state for subsequent tests or fixtures.\n",
      "    app.dependency_overrides.clear()\n",
      "\n",
      "--- Generating Refactored Tests ---\n",
      "\"\"\"\n",
      "Happy path tests for the /users endpoint using isolated database fixtures.\n",
      "\n",
      "This test file assumes a `conftest.py` provides a `client` fixture that\n",
      "handles database setup, teardown, and TestClient instantiation for each test,\n",
      "ensuring complete test isolation.\n",
      "\"\"\"\n",
      "import datetime\n",
      "from fastapi.testclient import TestClient\n",
      "\n",
      "\n",
      "def test_create_user_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests successful creation of a new user.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a prerequisite role for the user\n",
      "    role_payload = {\"role_name\": \"QA Engineer\"}\n",
      "    role_response = client.post(\"/roles\", json=role_payload)\n",
      "    assert role_response.status_code == 201\n",
      "    role_id = role_response.json()[\"role_id\"]\n",
      "\n",
      "    # Arrange: Define the user payload\n",
      "    user_payload = {\n",
      "        \"first_name\": \"Jane\",\n",
      "        \"last_name\": \"Doe\",\n",
      "        \"email\": \"jane.doe@example.com\",\n",
      "        \"role_id\": role_id,\n",
      "        \"start_date\": \"2024-07-29\"\n",
      "    }\n",
      "\n",
      "    # Act: Send a POST request to create the user\n",
      "    response = client.post(\"/users\", json=user_payload)\n",
      "\n",
      "    # Assert: Check the response\n",
      "    assert response.status_code == 201, f\"Expected 201, got {response.status_code}\"\n",
      "    response_data = response.json()\n",
      "    assert \"user_id\" in response_data\n",
      "    assert isinstance(response_data[\"user_id\"], int)\n",
      "    assert response_data[\"first_name\"] == user_payload[\"first_name\"]\n",
      "    assert response_data[\"last_name\"] == user_payload[\"last_name\"]\n",
      "    assert response_data[\"email\"] == user_payload[\"email\"]\n",
      "    assert response_data[\"role_id\"] == user_payload[\"role_id\"]\n",
      "    assert response_data[\"start_date\"] == user_payload[\"start_date\"]\n",
      "\n",
      "\n",
      "def test_list_users_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests successful retrieval of a list of all users.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a prerequisite role\n",
      "    role_payload = {\"role_name\": \"Developer\"}\n",
      "    role_response = client.post(\"/roles\", json=role_payload)\n",
      "    assert role_response.status_code == 201\n",
      "    role_id = role_response.json()[\"role_id\"]\n",
      "\n",
      "    # Arrange: Create two users to ensure the database is not empty\n",
      "    user1_payload = {\n",
      "        \"first_name\": \"Alice\",\n",
      "        \"last_name\": \"Smith\",\n",
      "        \"email\": \"alice.smith@example.com\",\n",
      "        \"role_id\": role_id,\n",
      "        \"start_date\": \"2024-01-10\"\n",
      "    }\n",
      "    user2_payload = {\n",
      "        \"first_name\": \"Bob\",\n",
      "        \"last_name\": \"Johnson\",\n",
      "        \"email\": \"bob.johnson@example.com\",\n",
      "        \"role_id\": role_id,\n",
      "        \"start_date\": \"2024-02-20\"\n",
      "    }\n",
      "    response1 = client.post(\"/users\", json=user1_payload)\n",
      "    response2 = client.post(\"/users\", json=user2_payload)\n",
      "    assert response1.status_code == 201\n",
      "    assert response2.status_code == 201\n",
      "    created_user1 = response1.json()\n",
      "    created_user2 = response2.json()\n",
      "\n",
      "    # Act: Send a GET request to list all users\n",
      "    response = client.get(\"/users\")\n",
      "\n",
      "    # Assert: Check the response\n",
      "    assert response.status_code == 200, f\"Expected 200, got {response.status_code}\"\n",
      "    users_list = response.json()\n",
      "    assert isinstance(users_list, list)\n",
      "    assert len(users_list) == 2\n",
      "\n",
      "    # Verify the created users are in the list\n",
      "    user_ids = {user[\"user_id\"] for user in users_list}\n",
      "    assert created_user1[\"user_id\"] in user_ids\n",
      "    assert created_user2[\"user_id\"] in user_ids\n",
      "    assert all(\n",
      "        key in user for user in users_list\n",
      "        for key in [\"user_id\", \"first_name\", \"last_name\", \"email\", \"role_id\", \"start_date\"]\n",
      "    )\n",
      "\n",
      "\n",
      "def test_get_user_by_id_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests successful retrieval of a single user by their ID.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a role and a user to fetch\n",
      "    role_payload = {\"role_name\": \"Product Manager\"}\n",
      "    role_response = client.post(\"/roles\", json=role_payload)\n",
      "    assert role_response.status_code == 201\n",
      "    role_id = role_response.json()[\"role_id\"]\n",
      "\n",
      "    user_payload = {\n",
      "        \"first_name\": \"Charlie\",\n",
      "        \"last_name\": \"Brown\",\n",
      "        \"email\": \"charlie.brown@example.com\",\n",
      "        \"role_id\": role_id,\n",
      "        \"start_date\": \"2023-11-01\"\n",
      "    }\n",
      "    create_response = client.post(\"/users\", json=user_payload)\n",
      "    assert create_response.status_code == 201\n",
      "    created_user_data = create_response.json()\n",
      "    user_id = created_user_data[\"user_id\"]\n",
      "\n",
      "    # Act: Send a GET request to retrieve the user by ID\n",
      "    response = client.get(f\"/users/{user_id}\")\n",
      "\n",
      "    # Assert: Check the response\n",
      "    assert response.status_code == 200, f\"Expected 200, got {response.status_code}\"\n",
      "    retrieved_user_data = response.json()\n",
      "    assert retrieved_user_data == created_user_data\n",
      "\n",
      "\n",
      "def test_update_user_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests successfully updating a user's details using PUT.\n",
      "    \"\"\"\n",
      "    # Arrange: Create initial and updated roles\n",
      "    initial_role_res = client.post(\"/roles\", json={\"role_name\": \"Intern\"})\n",
      "    updated_role_res = client.post(\"/roles\", json={\"role_name\": \"Full-Time Engineer\"})\n",
      "    assert initial_role_res.status_code == 201\n",
      "    assert updated_role_res.status_code == 201\n",
      "    initial_role_id = initial_role_res.json()[\"role_id\"]\n",
      "    updated_role_id = updated_role_res.json()[\"role_id\"]\n",
      "\n",
      "    # Arrange: Create an initial user\n",
      "    initial_user_payload = {\n",
      "        \"first_name\": \"David\",\n",
      "        \"last_name\": \"Copperfield\",\n",
      "        \"email\": \"david.c@example.com\",\n",
      "        \"role_id\": initial_role_id,\n",
      "        \"start_date\": \"2024-06-01\"\n",
      "    }\n",
      "    create_response = client.post(\"/users\", json=initial_user_payload)\n",
      "    assert create_response.status_code == 201\n",
      "    user_id = create_response.json()[\"user_id\"]\n",
      "\n",
      "    # Arrange: Define the updated payload\n",
      "    updated_user_payload = {\n",
      "        \"first_name\": \"Dave\",\n",
      "        \"last_name\": \"Copper\",\n",
      "        \"email\": \"dave.copper@example.com\",\n",
      "        \"role_id\": updated_role_id,\n",
      "        \"start_date\": \"2024-06-02\"\n",
      "    }\n",
      "\n",
      "    # Act: Send a PUT request to update the user\n",
      "    response = client.put(f\"/users/{user_id}\", json=updated_user_payload)\n",
      "\n",
      "    # Assert: Check the response\n",
      "    assert response.status_code == 200, f\"Expected 200, got {response.status_code}\"\n",
      "    response_data = response.json()\n",
      "    assert response_data[\"user_id\"] == user_id\n",
      "    assert response_data[\"first_name\"] == updated_user_payload[\"first_name\"]\n",
      "    assert response_data[\"last_name\"] == updated_user_payload[\"last_name\"]\n",
      "    assert response_data[\"email\"] == updated_user_payload[\"email\"]\n",
      "    assert response_data[\"role_id\"] == updated_user_payload[\"role_id\"]\n",
      "    assert response_data[\"start_date\"] == updated_user_payload[\"start_date\"]\n",
      "\n",
      "\n",
      "def test_delete_user_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests successfully deleting a user.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a role and a user to delete\n",
      "    role_payload = {\"role_name\": \"Temporary Staff\"}\n",
      "    role_response = client.post(\"/roles\", json=role_payload)\n",
      "    assert role_response.status_code == 201\n",
      "    role_id = role_response.json()[\"role_id\"]\n",
      "\n",
      "    user_payload = {\n",
      "        \"first_name\": \"Eve\",\n",
      "        \"last_name\": \"Delete\",\n",
      "        \"email\": \"eve.delete@example.com\",\n",
      "        \"role_id\": role_id,\n",
      "        \"start_date\": \"2024-07-01\"\n",
      "    }\n",
      "    create_response = client.post(\"/users\", json=user_payload)\n",
      "    assert create_response.status_code == 201\n",
      "    user_id = create_response.json()[\"user_id\"]\n",
      "\n",
      "    # Act: Send a DELETE request for the user\n",
      "    delete_response = client.delete(f\"/users/{user_id}\")\n",
      "\n",
      "    # Assert: Check the DELETE response status\n",
      "    assert delete_response.status_code == 204, f\"Expected 204, got {delete_response.status_code}\"\n",
      "\n",
      "    # Verify: Attempt to retrieve the deleted user\n",
      "    get_response = client.get(f\"/users/{user_id}\")\n",
      "\n",
      "    # Assert: Verify the user is gone\n",
      "    assert get_response.status_code == 404, \"User was not actually deleted.\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
    "db_fixture_prompt = f\"\"\"\n",
    "You are an expert QA Engineer specializing in pytest fixtures and FastAPI testing with database isolation. Your task is to generate a professional-grade pytest fixture for database testing.\n",
    "\n",
    "**Context - Application Code:**\n",
    "{app_code}\n",
    "\n",
    "**Instructions:**\n",
    "Generate a `conftest.py` file for pytest with the following requirements:\n",
    "\n",
    "1. **File Purpose:**\n",
    "   - This file should contain pytest fixtures that will be automatically discovered and used by all test files\n",
    "   - Include a comprehensive module-level docstring explaining that this provides shared fixtures for database isolation\n",
    "   - The fixture must create an isolated, in-memory SQLite database for testing\n",
    "\n",
    "2. **Required Imports:**\n",
    "   - Import necessary modules: `pytest`, `sqlalchemy` components, `fastapi.testclient.TestClient`\n",
    "   - Import the FastAPI `app`, database models (`Base`), and the `get_db` dependency from `app.main`\n",
    "   - Import any other necessary SQLAlchemy components for database creation\n",
    "\n",
    "3. **Database Fixture Requirements:**\n",
    "   \n",
    "   a) **Create a `test_db` fixture** with the following characteristics:\n",
    "      - Use `@pytest.fixture(scope=\"function\")` to create a fresh database for each test function\n",
    "      - Create an in-memory SQLite database using `sqlite:///:memory:`\n",
    "      - Use SQLAlchemy to create a new engine with proper configuration\n",
    "      - Create all database tables using `Base.metadata.create_all(bind=engine)`\n",
    "      - Create a SessionLocal for the test database\n",
    "      - Yield a database session to the test\n",
    "      - After the test completes, properly close the session and drop all tables using `Base.metadata.drop_all(bind=engine)`\n",
    "      - Include error handling to ensure cleanup happens even if tests fail\n",
    "   \n",
    "   b) **Create a `client` fixture** that:\n",
    "      - Depends on the `test_db` fixture (use `test_db` as a parameter)\n",
    "      - Overrides the FastAPI app's `get_db` dependency to use the test database instead of the production database\n",
    "      - Uses `app.dependency_overrides[get_db]` to replace the dependency\n",
    "      - Returns a `TestClient(app)` instance for making API requests\n",
    "      - After the test completes, clears the dependency override with `app.dependency_overrides.clear()`\n",
    "      - Properly handles setup and teardown\n",
    "\n",
    "4. **Best Practices to Follow:**\n",
    "   - Use proper fixture scopes (`function` scope ensures test isolation)\n",
    "   - Include try-finally blocks to ensure cleanup happens\n",
    "   - Add comprehensive docstrings to each fixture explaining its purpose and usage\n",
    "   - Ensure the fixture properly yields control to tests\n",
    "   - Make sure database sessions are properly closed\n",
    "   - Guarantee that each test starts with a clean, empty database\n",
    "   - Use the `yield` keyword correctly for setup/teardown patterns\n",
    "\n",
    "5. **Critical Implementation Details:**\n",
    "   - The `test_db` fixture must create a function that returns the test database session\n",
    "   - This function should be passed to `app.dependency_overrides[get_db]` in the client fixture\n",
    "   - Ensure compatibility with the existing `get_db` dependency pattern in the main application\n",
    "   - The fixture should handle SQLAlchemy session management properly\n",
    "   - Include proper error handling for database operations\n",
    "\n",
    "6. **Output Format:**\n",
    "   - Provide only valid Python code for the conftest.py file\n",
    "   - Do not include markdown code fences or explanatory text outside the code\n",
    "   - Ensure the code is properly formatted and follows PEP 8 style guidelines\n",
    "   - Include clear comments where helpful for understanding complex fixture behavior\n",
    "\n",
    "Generate the complete, production-ready conftest.py file now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# TODO: Write a prompt to refactor the happy path tests to use the new fixture.\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "You are an expert QA Engineer specializing in pytest and FastAPI testing. Your task is to refactor existing happy path tests to use a pytest fixture for database isolation.\n",
    "\n",
    "**Context - Application Code:**\n",
    "{app_code}\n",
    "\n",
    "**Instructions:**\n",
    "Generate a refactored pytest test file with the following requirements:\n",
    "\n",
    "1. **File Purpose:**\n",
    "   - This file contains the same happy path tests from Challenge 1, but refactored to use the database fixture from conftest.py\n",
    "   - Include a module-level docstring explaining that these are happy path tests using isolated database fixtures\n",
    "   - Tests should NOT create their own TestClient - they will use the fixture instead\n",
    "\n",
    "2. **Test Structure:**\n",
    "   - Use pytest best practices with clear, descriptive test function names following the pattern `test_<operation>_<expected_outcome>`\n",
    "   - Include proper imports: `pytest`, and import models/schemas if needed for type checking\n",
    "   - **DO NOT create a module-level TestClient** - instead, each test function should accept a `client` parameter\n",
    "   - The `client` parameter will automatically receive the fixture from conftest.py\n",
    "\n",
    "3. **Required Happy Path Tests (Refactored):**\n",
    "\n",
    "   a) **Test POST /users/** - Create a new user successfully:\n",
    "      - Function signature: `def test_create_user_success(client):`\n",
    "      - Accept `client` as a fixture parameter (pytest will inject it automatically)\n",
    "      - Send a valid user payload with all required fields (first_name, last_name, email, role_id, start_date)\n",
    "      - Assert the response status code is 201 (Created)\n",
    "      - Assert the response contains the created user data with a user_id\n",
    "      - Assert all input fields match the response fields\n",
    "      - Use a unique email address to ensure test isolation\n",
    "   \n",
    "   b) **Test GET /users/** - List all users:\n",
    "      - Function signature: `def test_list_users_success(client):`\n",
    "      - Accept `client` as a fixture parameter\n",
    "      - First, create one or two test users via POST to ensure the database has data\n",
    "      - Then send a GET request to retrieve all users\n",
    "      - Assert the response status code is 200 (OK)\n",
    "      - Assert the response is a list containing the created users\n",
    "      - Verify each user has the required fields (user_id, first_name, last_name, email, role_id, start_date)\n",
    "   \n",
    "   c) **Test GET /users/{{user_id}}** - Get a specific user:\n",
    "      - Function signature: `def test_get_user_by_id_success(client):`\n",
    "      - Accept `client` as a fixture parameter\n",
    "      - First create a user via POST /users/\n",
    "      - Extract the user_id from the response\n",
    "      - Then retrieve that user by their user_id\n",
    "      - Assert the response status code is 200 (OK)\n",
    "      - Assert the retrieved user data matches the created user data\n",
    "\n",
    "   d) **Test PUT /users/{{user_id}}** - Replace/update an entire user:\n",
    "      - Function signature: `def test_update_user_success(client):`\n",
    "      - Accept `client` as a fixture parameter\n",
    "      - First create a user\n",
    "      - Then send a PUT request with completely new data for all fields\n",
    "      - Assert the response status code is 200 (OK)\n",
    "      - Assert all fields are updated to the new values\n",
    "\n",
    "   e) **Test DELETE /users/{{user_id}}** - Delete a user:\n",
    "      - Function signature: `def test_delete_user_success(client):`\n",
    "      - Accept `client` as a fixture parameter\n",
    "      - First create a user\n",
    "      - Then send a DELETE request for that user_id\n",
    "      - Assert the response status code is 204 (No Content)\n",
    "      - Verify the user is actually deleted by trying to GET it (should return 404)\n",
    "\n",
    "4. **Key Differences from Original Tests:**\n",
    "   - Each test function MUST accept `client` as a parameter (this is the fixture injection)\n",
    "   - NO module-level `client = TestClient(app)` - the fixture handles this\n",
    "   - Tests can assume they start with a completely empty, fresh database\n",
    "   - Tests don't need to worry about existing data or conflicts (except within the same test)\n",
    "   - Each test runs in isolation with its own database\n",
    "\n",
    "5. **Best Practices to Follow:**\n",
    "   - Each test is independent and starts with a clean database (thanks to the fixture)\n",
    "   - Use descriptive variable names\n",
    "   - Add docstrings to each test function explaining what it tests\n",
    "   - Use proper assertions with clear failure messages\n",
    "   - Follow the Arrange-Act-Assert (AAA) pattern\n",
    "   - For tests that need data, create it at the start of the test\n",
    "\n",
    "6. **Output Format:**\n",
    "   - Provide only valid Python code for the test file\n",
    "   - Do not include markdown code fences or explanatory text outside the code\n",
    "   - Include a module-level docstring explaining the purpose of the test file\n",
    "   - Ensure the code is properly formatted and follows PEP 8 style guidelines\n",
    "\n",
    "Generate the complete, production-ready pytest test file with fixture-based tests now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\", overwrite= True)\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: View the API in Browser\n",
    "\n",
    "You can view the FastAPI application's interactive documentation in your browser. The application should already be running from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import time\n",
    "\n",
    "# API Base URL\n",
    "api_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "# FastAPI automatically provides interactive API documentation\n",
    "# Swagger UI (interactive API docs)\n",
    "swagger_url = f\"{api_url}/docs\"\n",
    "\n",
    "# ReDoc (alternative API docs)\n",
    "redoc_url = f\"{api_url}/redoc\"\n",
    "\n",
    "print(\"Opening FastAPI application in browser...\")\n",
    "print(f\"\\nAPI Base URL: {api_url}\")\n",
    "print(f\"Swagger UI (Interactive Docs): {swagger_url}\")\n",
    "print(f\"ReDoc (Alternative Docs): {redoc_url}\")\n",
    "print(\"\\nNote: Make sure your FastAPI server is running in the terminal!\")\n",
    "print(\"If not running, execute this command in the terminal:\")\n",
    "print(\"  cd C:\\\\Users\\\\labadmin\\\\Documents\\\\AG-AISOFTDEV\")\n",
    "print(\"  python -m uvicorn app.main:app --reload\")\n",
    "\n",
    "# Open Swagger UI in the default browser\n",
    "webbrowser.open(swagger_url)\n",
    "\n",
    "print(\"\\n Browser opened with Swagger UI\")\n",
    "print(\"\\nYou can:\")\n",
    "print(\"  - View all available endpoints\")\n",
    "print(\"  - Test the API directly from the browser\")\n",
    "print(\"  - See request/response schemas\")\n",
    "print(\"  - Try out POST, GET, PUT, DELETE operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Tests\n",
    "\n",
    "Now let's run the generated tests to verify they work correctly. We'll run them with pytest and check for any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running Tests with Pytest ===\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running: Happy Path Tests (Simple)\n",
      "File: tests/test_main_simple.py\n",
      "============================================================\n",
      "\n",
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.9, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\labadmin\\Documents\\AG-AISOFTDEV\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\labadmin\\Documents\\AG-AISOFTDEV\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.11.0, langsmith-0.4.38, asyncio-1.2.0\n",
      "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "tests/test_main_simple.py::test_create_user_success \u001b[32mPASSED\u001b[0m\u001b[33m               [ 33%]\u001b[0m\n",
      "tests/test_main_simple.py::test_get_users_list_success \u001b[32mPASSED\u001b[0m\u001b[33m            [ 66%]\u001b[0m\n",
      "tests/test_main_simple.py::test_get_specific_user_success \u001b[32mPASSED\u001b[0m\u001b[33m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== warnings summary ===============================\u001b[0m\n",
      "app\\main.py:60\n",
      "  c:\\Users\\labadmin\\Documents\\AG-AISOFTDEV\\app\\main.py:60: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "    class UserRead(UserBase):\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.20s\u001b[0m\u001b[33m =========================\u001b[0m\n",
      "\n",
      "\n",
      " Happy Path Tests (Simple) - ALL TESTS PASSED\n",
      "\n",
      "============================================================\n",
      "Running: Database Fixture Tests\n",
      "File: tests/test_main_with_fixture.py\n",
      "============================================================\n",
      "\n",
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.9, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\labadmin\\Documents\\AG-AISOFTDEV\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\labadmin\\Documents\\AG-AISOFTDEV\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.11.0, langsmith-0.4.38, asyncio-1.2.0\n",
      "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items\n",
      "\n",
      "tests/test_main_with_fixture.py::test_create_user_success \u001b[32mPASSED\u001b[0m\u001b[33m         [ 20%]\u001b[0m\n",
      "tests/test_main_with_fixture.py::test_list_users_success \u001b[32mPASSED\u001b[0m\u001b[33m          [ 40%]\u001b[0m\n",
      "tests/test_main_with_fixture.py::test_get_user_by_id_success \u001b[32mPASSED\u001b[0m\u001b[33m      [ 60%]\u001b[0m\n",
      "tests/test_main_with_fixture.py::test_update_user_success \u001b[32mPASSED\u001b[0m\u001b[33m         [ 80%]\u001b[0m\n",
      "tests/test_main_with_fixture.py::test_delete_user_success \u001b[32mPASSED\u001b[0m\u001b[33m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== warnings summary ===============================\u001b[0m\n",
      "app\\main.py:60\n",
      "  c:\\Users\\labadmin\\Documents\\AG-AISOFTDEV\\app\\main.py:60: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "    class UserRead(UserBase):\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m5 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.19s\u001b[0m\u001b[33m =========================\u001b[0m\n",
      "\n",
      "\n",
      " Database Fixture Tests - ALL TESTS PASSED\n",
      "\n",
      "============================================================\n",
      "Test verification complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=== Running Tests with Pytest ===\\n\")\n",
    "\n",
    "# Test files to run\n",
    "test_targets = [\n",
    "    (\"Happy Path Tests (Simple)\", \"tests/test_main_simple.py\"),\n",
    "    (\"Database Fixture Tests\", \"tests/test_main_with_fixture.py\"),\n",
    "]\n",
    "\n",
    "for test_name, test_file in test_targets:\n",
    "    full_path = os.path.join(project_root, test_file)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"\\n {test_name}\")\n",
    "        print(f\"   File not found: {test_file}\")\n",
    "        print(\"   Skipping...\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {test_name}\")\n",
    "    print(f\"File: {test_file}\")\n",
    "    print('='*60 + \"\\n\")\n",
    "    \n",
    "    # Run pytest with verbose output\n",
    "    cmd = [sys.executable, \"-m\", \"pytest\", test_file, \"-v\", \"--tb=short\"]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            cwd=project_root,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        # Show output\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "        \n",
    "        # Check result\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\n {test_name} - ALL TESTS PASSED\")\n",
    "        else:\n",
    "            print(f\"\\n {test_name} - SOME TESTS FAILED (Exit code: {result.returncode})\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\" {test_name} - TIMEOUT (tests took too long)\")\n",
    "    except Exception as e:\n",
    "        print(f\" {test_name} - ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test verification complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
