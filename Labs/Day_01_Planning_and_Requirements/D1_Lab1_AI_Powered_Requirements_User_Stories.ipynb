{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Lab 1: AI-Powered Requirements & User Stories\n",
    "\n",
    "**Objective:** Use a Large Language Model (LLM) to decompose a vague problem statement into structured features, user personas, and Agile user stories, culminating in a machine-readable JSON artifact.\n",
    "\n",
    "**Estimated Time:** 90 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to the first hands-on lab of the AI-Driven Software Engineering Program! All great software projects begin with a clear understanding of the problem to be solved. In this lab, you will take on the role of a tech lead or product manager and use an LLM as a co-pilot to transform a simple, high-level problem into a set of well-defined, actionable requirements. This process is fundamental to ensuring that the team builds the *right* product.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "This initial block sets up our environment. It adds the project's root directory to the Python path, allowing us to import our custom `utils.py` script. We then initialize the connection to our Large Language Model (LLM).\n",
    "\n",
    "**Model Selection:**\n",
    "Our `utils.py` script is configured to work with multiple AI providers. You can change the `model_name` parameter in the `setup_llm_client()` function to any of the models listed in the `RECOMMENDED_MODELS` dictionary in `utils.py`. For example, to use a Hugging Face model, you could change the line to: `client, model_name, api_provider = setup_llm_client(model_name=\"meta-llama/Llama-3.3-70B-Instruct\")`\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client for our chosen LLM.\n",
    "- `get_completion()`: To send a prompt to the LLM and get a response.\n",
    "- `save_artifact()`: To save our generated requirements to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 16:46:09,751 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    # Assumes the notebook is in 'labs/Day_01_.../'\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    # Fallback for different execution environments\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact\n",
    "\n",
    "# Initialize the LLM client. You can change the model here.\n",
    "# For example: setup_llm_client(model_name=\"gemini-2.5-flash\")\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Problem Statement\n",
    "\n",
    "Every project starts with a problem. Our problem is a common one in many organizations:\n",
    "\n",
    "> **\"We need a tool to help our company's new hires get up to speed.\"**\n",
    "\n",
    "This statement is intentionally vague. Our job is to use the LLM to add structure and detail to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = \"We need a tool to help our company's new hires get up to speed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges\n",
    "\n",
    "Complete the following challenges in order. Each one builds upon the last, increasing in technical complexity and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Brainstorming Features\n",
    "\n",
    "**Task:** Use the LLM to brainstorm a list of potential features and user personas based on the problem statement.\n",
    "\n",
    "**Instructions:**\n",
    "1. Write a simple prompt that asks the LLM to brainstorm features for the onboarding tool.\n",
    "2. Write a second prompt to identify three distinct user personas who would use this tool.\n",
    "3. Run both prompts and review the markdown output.\n",
    "\n",
    "**Expected Quality:** The output should be a simple, readable markdown list of features and a description of the personas. This is a good first step but lacks the structure needed for automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Brainstorming Features ---\n"
     ]
    }
   ],
   "source": [
    "# TCreate a string variable named 'features_prompt'.\n",
    "# This prompt should ask the LLM to brainstorm features based on the problem_statement.\n",
    "features_prompt = \"\"\" Brainstorm features for the onboarding tool that \n",
    "would help our company's new hires get up to speed \"\"\"\n",
    "\n",
    "print(\"--- Brainstorming Features ---\")\n",
    "brainstormed_features = get_completion(features_prompt, client, model_name, api_provider)\n",
    "print(brainstormed_features)\n",
    "\n",
    "# TCreate a string variable named 'personas_prompt'.\n",
    "# This prompt should ask the LLM to identify three user personas based on the problem_statement.\n",
    "personas_prompt = \"\"\" Identify three distinct user personas who would use \n",
    "the onboarding tool  \"\"\"\n",
    "\n",
    "print(\"\\n--- Identifying User Personas ---\")\n",
    "user_personas = get_completion(personas_prompt, client, model_name, api_provider)\n",
    "print(user_personas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Formal User Stories\n",
    "\n",
    "**Task:** Now, let's increase the value by generating structured, formal Agile User Stories.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create a new, more sophisticated prompt.\n",
    "2. This prompt should instruct the LLM to act as a Senior Product Manager.\n",
    "3. It must use the brainstormed features and personas from the previous step as context.\n",
    "4. The key instruction is to generate a list of user stories, each with detailed acceptance criteria in Gherkin format (`Given/When/Then`).\n",
    "5. **Crucially, the prompt must demand the final output be a well-formed JSON array of objects.** Each object should represent a user story and have keys like `id`, `user_story`, `persona`, and `acceptance_criteria`.\n",
    "\n",
    "> **Tip:** If the LLM's output isn't perfect JSON, try making your prompt even more specific. You can tell it, 'Do not include any text before or after the JSON array. Your response must begin with [ and end with ].'\n",
    "\n",
    "**Expected Quality:** The output should not be markdown, but a clean, parsable JSON string. This is a significant step up in value, as a JSON artifact can be automatically processed by other systems (e.g., imported into Jira)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating User Stories as JSON ---\n",
      "Successfully parsed LLM output as JSON.\n",
      "\n",
      "--- Sample User Story ---\n",
      "{\n",
      "  \"id\": 1,\n",
      "  \"persona\": \"Alex Chen\",\n",
      "  \"user_story\": \"As an Anxious New Hire, I want to complete all my HR paperwork digitally before my first day, so that I can focus on meeting my team and learning my role when I start.\",\n",
      "  \"acceptance_criteria\": [\n",
      "    \"Given I am a new hire who has received my pre-boarding link, When I log in for the first time, Then I am directed to a secure document hub.\",\n",
      "    \"Given I am in the document hub, When I open a form like a W-4 or I-9, Then I can fill out all required fields directly in the browser.\",\n",
      "    \"Given a form requires a signature, When I click the signature field, Then I can use an e-signature tool to sign the document.\",\n",
      "    \"Given I have submitted all my documents, When I view my main onboarding checklist, Then the 'Complete Paperwork' task is marked as complete.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# TCreate a detailed prompt string named 'json_user_stories_prompt'.\n",
    "# This prompt needs to instruct the LLM to act as a Senior Product Manager and convert the\n",
    "# brainstormed features and personas into a structured JSON array of user stories.\n",
    "# Tip: Be very specific about the required JSON format in your prompt instructions. Tell it what keys to use and what the data types should be.\n",
    "json_user_stories_prompt = f\"\"\" \n",
    "You are a Senior Product Manager creating a product backlog for a new hire onboarding tool.\n",
    "\n",
    "Based on the following context:\n",
    "<context>\n",
    "Problem Statement: {problem_statement}\n",
    "Potential Features: {brainstormed_features}\n",
    "User Personas: {user_personas}\n",
    "</context>\n",
    "\n",
    "Your task is to generate a list of 5 detailed user stories.\n",
    "\n",
    "**OUTPUT REQUIREMENTS**:\n",
    "- You MUST output a valid JSON array. Your response must begin with [ and end with ]. Do not include any text or markdown before or after the JSON array.\n",
    "- Each object in the array must represent a single user story.\n",
    "- Each object must have the following keys: 'id' (an integer), 'persona' (a string from the personas), 'user_story' (a string in the format 'As a [persona], I want [goal], so that [benefit].'), and 'acceptance_criteria' (an array of strings, with each string in Gherkin format 'Given/When/Then').\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"--- Generating User Stories as JSON ---\")\n",
    "json_output_str = get_completion(json_user_stories_prompt, client, model_name, api_provider, temperature=0.2)\n",
    "\n",
    "# Let's try to parse the JSON to see if the LLM followed instructions\n",
    "try:\n",
    "    # The LLM might wrap the JSON in markdown fences (```json ... ```).\n",
    "    # We'll clean that up before parsing.\n",
    "    if '```' in json_output_str:\n",
    "        json_output_str = json_output_str.split('```')[1].lstrip('json').strip()\n",
    "    \n",
    "    user_stories_json = json.loads(json_output_str)\n",
    "    print(\"Successfully parsed LLM output as JSON.\")\n",
    "    \n",
    "    if user_stories_json:\n",
    "        print(\"\\n--- Sample User Story ---\")\n",
    "        print(json.dumps(user_stories_json[0], indent=2))\n",
    "    else:\n",
    "        print(\"JSON array is empty.\")\n",
    "\n",
    "except (json.JSONDecodeError, TypeError, IndexError) as e:\n",
    "    print(f\"Error: Failed to parse LLM output as JSON. Error: {e}\")\n",
    "    print(\"LLM Output was:\\n\", json_output_str)\n",
    "    user_stories_json = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Programmatic Validation and Artifact Creation\n",
    "\n",
    "**Task:** Now for the highest-value step. Instead of just looking at the JSON, we will programmatically validate it and save it as a formal project artifact. This ensures reliability and prepares the requirements for automated use in later stages of the SDLC.\n",
    "\n",
    "**Instructions:**\n",
    "1. Complete the `validate_and_save_stories` function below.\n",
    "2. The function should iterate through the list of stories.\n",
    "3. For each story, it must validate that the required keys are present and that the acceptance criteria list is not empty.\n",
    "4. If all stories are valid, it should save the data to `artifacts/day1_user_stories.json`.\n",
    "\n",
    "**Expected Quality:** A robust script that guarantees the integrity of our requirements artifact. The final output is a validated `day1_user_stories.json` file in the `artifacts` directory, ready to be used as a reliable input for Day 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All user stories passed validation.\n"
     ]
    }
   ],
   "source": [
    "def validate_and_save_stories(stories_data):\n",
    "    \"\"\"Validates the structure of the user stories data and saves it if valid.\"\"\"\n",
    "    if not isinstance(stories_data, list) or not stories_data:\n",
    "        print(\"Validation Failed: Data is not a non-empty list.\")\n",
    "        return\n",
    "\n",
    "    required_keys = ['id', 'persona', 'user_story', 'acceptance_criteria']\n",
    "    all_stories_valid = True\n",
    "\n",
    "    # TImplement the validation logic inside this function.\n",
    "    # 1. Loop through each story in the 'stories_data' list.\n",
    "    # 2. For each story, check if it contains all the 'required_keys'.\n",
    "    # 3. Also check if the 'acceptance_criteria' list is not empty.\n",
    "    # 4. If a story is invalid, print an error message and set 'all_stories_valid' to False.\n",
    "    #    (You can use 'continue' to skip to the next story).\n",
    "\n",
    "    # Your validation code here\n",
    "\n",
    "    # SLoop through each story object in the list.\n",
    "    for i, story in enumerate(stories_data):\n",
    "        # Check for the presence of all required keys.\n",
    "        if not all(key in story for key in required_keys):\n",
    "            print(f\"Validation Failed: Story at index {i} is missing one or more required keys.\")\n",
    "            print(f\"   Expected keys: {required_keys}\")\n",
    "            print(f\"   Found keys: {list(story.keys()) if isinstance(story, dict) else 'Not a dictionary'}\")\n",
    "            all_stories_valid = False\n",
    "            continue # Don't bother with further checks for this invalid story\n",
    "        \n",
    "        # Check that the acceptance criteria is a list with at least one item.\n",
    "        ac = story.get('acceptance_criteria')\n",
    "        if not isinstance(ac, list) or not ac:\n",
    "            print(f\"Validation Failed: Story at index {i} (ID: '{story.get('id')}') has invalid or empty acceptance criteria.\")\n",
    "            print(f\"   Expected: list with at least one item\")\n",
    "            print(f\"   Found: {type(ac)} with value {ac}\")\n",
    "            all_stories_valid = False\n",
    "\n",
    "    if all_stories_valid:\n",
    "        print(\"\\nAll user stories passed validation.\")\n",
    "        artifact_path = \"artifacts/day1_user_stories.json\"\n",
    "        \n",
    "        # TCall the save_artifact function from utils.py to save the data.\n",
    "        # Remember to convert the Python list back to a JSON string using json.dumps().\n",
    "        save_artifact(json.dumps(stories_data, indent=2), artifact_path)\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print(\"\\nValidation failed. Artifact not saved.\")\n",
    "\n",
    "# Run the validation on the JSON data from the previous step\n",
    "if 'user_stories_json' in locals() and user_stories_json:\n",
    "    validate_and_save_stories(user_stories_json)\n",
    "else:\n",
    "    print(\"Skipping validation as user_stories_json is empty or not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Congratulations! You have completed the first lab. You started with a vague, one-sentence problem and finished with a structured, validated, machine-readable requirements artifact. This is the critical first step in an AI-assisted software development lifecycle. The `day1_user_stories.json` file you created will be the direct input for our next lab, where we will generate a formal Product Requirements Document (PRD).\n",
    "\n",
    "> **Key Takeaway:** The single most important skill demonstrated in this lab is turning unstructured ideas into structured, machine-readable data (JSON). This transformation is what enables automation and integration with other tools later in the SDLC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
